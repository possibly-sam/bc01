---
title: "bc04"
author: "Phillip Abbott"
date: "Tue 24 Sep XIX"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

too many words..trying to see if I can get an intuitive understanding with fewer words.

```{r}


library(tidyverse)



library(tidytext)
library(tm)
library(e1071)
library(odbc)

library(NLP)
# library(openNLP)

library(odbc)

# cloud version
con <- dbConnect(odbc::odbc(), .connection_string = "Driver={PostgreSQL Unicode};Server=mcfc01.c4hbjfyzingo.eu-west-2.rds.amazonaws.com;Database=mcfc01;UID=mcfcuser01;PWD=projecthack01")

# local copy
# con <- dbConnect(odbc::odbc(), .connection_string = "Driver={PostgreSQL Unicode};Database=usit_alpha")


```



```{sql connection=con,  output.var="business_cases_year_17"}

SELECT "Brief_Summary" ::varchar(4096), 
  uui01::bigint*1000000000+uui02 AS uui, 
  
  "CIO_Evaluation_Color" from public.business_case_alpha WHERE year=17

```





# re-calculate, but with massively reduced number of words under consideration.



## Calculation
skip this section if not re-calculating, and just load the result

```{r}

source("pcx01.R")

p5 <- pcx(business_cases_year_17$uui, business_cases_year_17$Brief_Summary, function(x) 10)

# : my_corups
# : my_document_term_matrix
# : my_incidence_matrix
# : my_principal_component_matrix
# : my_tidy_dtm


# p0$project_onto_principal_component(2) - p0$project_onto_principal_component(1)
p5$my_projection_matrix %>% View()

# p0$plot_word_counts()
# p0 %>% ls.str()




the_categories <- rep(1, nrow(business_cases_year_17))
the_categories[ business_cases_year_17$CIO_Evaluation_Color=="Green"] <- 0

save(p5,the_categories,  file="p5.RData")

```

```{r}
source("pcx01.R")

source("test_model.R")
source("some_table_information.R")
source("roc_info.R")

load("p5.RData")

q0 <- p5$my_incidence_matrix  %>% cbind(the_categories)
q1 <- p5$my_projection_matrix %>% cbind(the_categories)
# p5$my_incidence_matrix %>% str()

```



```{r}
source("some_table_information.R")

# (idx, ..cols.., category)

ppcx <- function(thing_produced_from_pcx, the_categories) {
  
  result <- new.env(emptyenv())
  
  result$my_incidence_matrix <- thing_produced_from_pcx$my_incidence_matrix %>% cbind(the_categories)
  result$my_projection_matrix <- thing_produced_from_pcx$my_projection_matrix %>% cbind(the_categories)
  
  result$get_incidence_vector <- function(n) result$my_incidence_matrix[,n+1 ]
  result$get_projection_vector <- function(n) result$my_projection_matrix[,n+1 ]
  
  result$plot_projection <- function(xaxis, yaxis) {
    plot(result$my_projection_matrix[,yaxis+1] ~ result$my_projection_matrix[,xaxis+1], col=result$my_projection_matrix$the_categories+1)
  }

  result$plot_incidence <- function(xaxis, yaxis) {
    plot(result$my_incidence_matrix[,yaxis+1] ~ result$my_incidence_matrix[,xaxis+1], col=result$my_incidence_matrix$the_categories+1)
  }
  
  result$incidence_table <- function(xaxis, yaxis) {
    table( result$get_incidence_vector (yaxis), result$get_incidence_vector (xaxis) )
  }  
  
  result$get_incidence_logistic_model <- function() {
    glm(the_categories~.,data=result$my_incidence_matrix[,-1], family=binomial)
  }
    
  result$get_projection_logistic_model <- function() {
    glm(the_categories~.,data=result$my_projection_matrix[,-1], family=binomial)
  }

  result$get_test_model_confusion_matrix <- function(the_proportion, the_cutoff)  {
    x0 <- test_model(the_categories~., result$my_projection_matrix[,-1],the_proportion)
    x1 <- x0$test$pred > the_cutoff
    table(actual=x0$test$the_categories, predicted=x1)
  }
  
  result$get_test_model_confusion_matrix_information <- function(the_proportion, the_cutoff) {
    (result$get_test_model_confusion_matrix(the_proportion, the_cutoff) %>% some_table_information() )$ summary()
  }
  
  
  result$get_incidence_test_model_confusion_matrix <- function(the_proportion, the_cutoff)  {
    x0 <- test_model(the_categories~., result$my_incidence_matrix[,-1],the_proportion)
    x1 <- x0$test$pred > the_cutoff
    table(actual=x0$test$the_categories, predicted=x1)
  }
  
  result$get_incidence_test_model_confusion_matrix_information <- function(the_proportion, the_cutoff) {
    (result$get_incidence_test_model_confusion_matrix(the_proportion, the_cutoff) %>% some_table_information() )$ summary()
  }
  
  result$get_roc_info <- function(the_proportion, the_cutoff) {
   ( result$get_test_model_confusion_matrix(the_proportion, the_cutoff) %>% some_table_information() )$ roc
    
  }


  result
  
  
}

```

```{r}

source("pcx01.R")
pp0 <- ppcx(p5,the_categories )

pp0$get_incidence_logistic_model() %>% summary()

table(data=pp0$my_incidence_matrix$data,naughty= pp0$my_incidence_matrix$the_categories) %>% barplot()

pp0$get_incidence_logistic_model() %>% summary()

```


```{r}

dt0 <- table(data=pp0$my_incidence_matrix$data,naughty= pp0$my_incidence_matrix$the_categories)

round(100*dt0/sum(dt0))

```


```{r}

pp0$get_projection_logistic_model() %>% summary()

dt1 <- table(data=pp0$my_projection_matrix$pcx_1,naughty= pp0$my_projection_matrix$the_categories)

round(100*dt1/sum(dt1))

boxplot(pp0$my_projection_matrix$pcx_4~pp0$my_projection_matrix$the_categories)

```

Given a cutoff for pcx_4 to mark as predicted positive, what are the false pos/false neg rates?


x0 <- data.frame( t(pp0$roc(the_categories~pcx_4, 0) ))
for (k in 1:100) x0 <- x0 %>% rbind( t(pp0$roc(the_categories~pcx_4, k/100)))

plot(x0$true_positive_rate~x0$false_positive_rate)
abline(0,1)

```{r}

source("pcx01.R")

pp0 <- ppcx(p5,the_categories )

# hm0 <- pp0$get_projection_logistic_model_formula(the_categories~pcx_4)

# x0 <- pp0$get_projection_logistic_model_formula_training_prediction_probabilities(the_categories~pcx_4, 0.2)

# (table(actual=x0$actual,predicted=x0$predicted) %>% some_table_information())$roc

x0 <- pp0$roc_all(the_categories~pcx_4)

x0 <- x0 %>% cbind( meas=(1-x0$false_positive_rate)^2*x0$true_positive_rate^2)

x0 <- x0[15:44,]
x0 <- x0[x0$false_positive_rate < .43,]
x0 <- x0[x0$true_positive_rate > .18,]

plot(x0$meas~x0$cutoff)

pp0$plot_roc_all(the_categories~pcx_4)

abline(v=.43)
abline(h=.18)

x1 <- pp0$get_probs(the_categories~pcx_4)

```






```{r}
xa <- test_model(the_categories~., pp0$my_incidence_matrix[,-1],.9)

xa$test
x1 <- xa$test$pred > .2

table(xa$test$the_categories, x1)

```





```{r}

pp0 <- ppcx(p5,the_categories )

# pp0$plot_projection(1,2)
# pp0$incidence_table(1,2)

# pp0$get_test_model_confusion_matrix_information(0.9, 0.2)

# xb <-  pp0$get_test_model_confusion_matrix(.9, .2) %>% some_table_information() 

xb <- data.frame( t(pp0$get_roc_info(0.8, 0)) )
for (k in 1:100) xb <- xb %>% rbind( data.frame( t(pp0$get_roc_info(0.9, k/100))))

plot(xb$true_positive_rate~xb$false_positive_rate)
abline(0,1)

```


```{r}

pp0$get_incidence_logistic_model() %>% summary()

pp0$get_projection_logistic_model() %>% summary()
```

Is using 'data' or 'pcx2' better than guessing, and if so by how much?



```{r}



x0 <- test_model(the_categories~., pp0$my_projection_matrix[,-1],.9)



```








```{r}


table(q0$management,q0$the_categories)
cov(q0$management,q0$the_categories)

cor(q0[,-1])



hm <- glm(the_categories~p5$my_projection_matrix$pcx_1, family=binomial)

hm %>% summary()

# if there word 'data' appears, there is a slightly greater possibility of being in the red.
# AIC: 879.74
# with just data + system, AIC 874.93
# with just data, AIC = 873.77

#, with pcx1, AIC = 877.95



```






## Load the result

```{r}

source("pcx01.R")


load("p0.RData")



```



# can we find a correlation?

I did a naive logistic regression on 'green or not' against the first few dozen principal components, a few at a time.  I threw out all the variables that R didn't show as significant with at least a '.'.  This is the list I was left with.  The next question is:  how good is this model, in terms of the ROC curve?  Can we improve it by taking out terms or adding other information (e.g. department).  To answer that question, one needs to calculate the ROC curve, which involves evaluating the prediction at different levels of probability.

```{r}

some_nice_pcx <- c(1,2,4,5, 9, 11, 18, 25, 33, 40, 48, 58, 60, 73, 76, 79, 95, 96, 97, 115, 116, 118)

the_categories <- rep(1, nrow(business_cases_year_17))
the_categories[ business_cases_year_17$CIO_Evaluation_Color=="Green"] <- 0


p2 <- p0$my_projection_matrix[,(some_nice_pcx+1)] %>% cbind(the_categories)

p2$the_categories <- p2$the_categories

hm <- glm(the_categories~., data=p2, family=binomial)

summary(hm)

# 1 2 4
# 1 2 4 9 18 60 95 97

```

after we remove the ones with no significance, some of them lose significance again.


# plot 2 of the pcx against each other

```{r}

plot(p5$my_projection_matrix[,2]~p5$my_projection_matrix[,3], col= the_categories+1)

```





```{r}


some_nice_pcx <- c(1,2,4, 9, 18,  60, 95, 97)

p2 <- p0$my_projection_matrix[,(some_nice_pcx+1)] %>% cbind(the_categories)


hm <- glm(the_categories~., data=p2, family=binomial)

summary(hm)




```





```{r}

source("test_model.R")
source("some_table_information.R")
source("roc_info.R")

hyper <- function(the_model, the_data, the_proportion) {
  result <- new.env(emptyenv())
  
    t0 <- test_model(the_categories~., p2, 0.9)
    t1 <- t0$test[,c("the_categories", "pred")]

    x0 <- roc_info_all( t1$the_categories, t1$pred )

    result$plot_me <- function() x0$plot_me()
    result$get_auc <- function() x0$get_auc()

    result

}

# we assume the dependent variable is in the last column
get_random_sample<- function(the_data) {
  n <- (the_data %>% ncol()) - 1
  how_many <- sample(1:(n-1), 1)
  the_sample <- sample(1:n, how_many)
  the_sample
}

asitd <- data.frame( trs=p2 %>% get_random_sample()  %>% reduce(paste), auc=0 )

# need to take a non-trivial number of samples with a given set of parameters

# dependent variable is called the_category and is in the last column
get_auc_distribution <- function( some_nice_random_sample, the_data, the_proportion = 0.9, the_distribution_size = 30) {
  
  1:the_distribution_size %>% map (~hyper(the_categories~., the_data[,-some_nice_random_sample], the_proportion)$get_auc() ) %>% unlist()
  
}

trs80 <- p2 %>% get_random_sample()
x0 <- trs80 %>% get_auc_distribution(p2)

x0 %>% summary()

x0 %>% boxplot(x0)
```


# taking p2 with its reduced list -- just 8 columns


```{r}

x0 <- 1:8 %>% get_auc_distribution(p2)


x1 <- c(1,2,4,5,6) %>% get_auc_distribution(p2)

x2 <-  c(2,5) %>% get_auc_distribution(p2)
x0 %>% summary()
x1 %>% summary()
x2 %>% summary()

 boxplot(x0, x1, x2)

```

principal component 18 seems to have magical properties.  putting it in with pc2 gives us about as cood auc as anything else.




# this is a heavy calculation.

```{r}

some_submodels <- list()

for (k in 1:22) {
  the_sample <- 1:k
  some_submodels[[k]] <- the_sample %>% get_auc_distribution(p2)
}


```





```{r}

boxplot( some_submodels[[1]],  some_submodels[[2]],  some_submodels[[3]],  some_submodels[[4]],
         some_submodels[[5]],  some_submodels[[6]],  some_submodels[[7]],  some_submodels[[8]],
         some_submodels[[9]],  some_submodels[[10]], some_submodels[[11]], some_submodels[[12]],
         some_submodels[[13]], some_submodels[[14]], some_submodels[[15]], some_submodels[[16]],
         some_submodels[[17]], some_submodels[[18]], some_submodels[[19]], some_submodels[[20]])

```

```{r}
idx <- 1:21
mmean <- idx %>% map(~some_submodels[[.]] %>% mean()) %>% unlist()
mmead <- idx %>% map(~some_submodels[[.]] %>% median()) %>% unlist()

mdf <- data.frame(idx, mmean, mmead)

mdf[order(mdf$mmean, decreasing=TRUE), ]

plot(mmead~mmean,data=mdf)

```

? Maybe including the first 3 would be an acceptable compromise over everything?






 Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.5033  0.5885  0.6232  0.6374  0.6882  0.7870 


for (k in 1:100) {
  trs <- p2 %>% get_random_sample()
  auc <- hyper(the_categories~., p2[,-trs], 0.9)$get_auc()
  trs_ <- (1:(ncol(p2)-1) %>% setdiff (trs)  ) %>% reduce(paste)
  
  if (.8 < auc)
  asitd <- asitd %>% rbind( t( c(trs=trs_ , auc=auc)))
}

asitd <- asitd[order(asitd$auc),]

# h0 <- hyper(the_categories~., p2[,-1:2], 0.9)

# h0$plot_me()
# h0$get_auc()




Some models with good AUC:

10 14 17

1 3 4 6 7 8 9 11 12 13 14 15 16 17 18 19 20 21 22

```{r}




 h0 <- hyper(the_categories~., p2[,c(1, 3, 4, 6, 7, 8, 9, 11:22)], 0.9)
h0$plot_me()
h0$get_auc()

```






1 16
0.800909090909091
10
11 5 21 19 16 14 9 2 17 3 6 8 20 10 13 22 15 18 7


t0 <- test_model(the_categories~., p2, 0.9)
t1 <- t0$test[,c("the_categories", "pred")]
# x0_ <- roc_info( t1$the_categories, t1$pred, .5)
x0 <- roc_info_all( t1$the_categories, t1$pred )


# plot(true_positive_rate~false_positive_rate, data=x0$df)
# abline(0,1)
x0$plot_me()

x0$get_auc()



roc01 <- x0$df[order(x0$df$false_positive_rate),]
auc <- 0
for (k in 2:nrow(roc01)) {
    x_0 <- roc01[k-1, "false_positive_rate"] 
    x_1 <- roc01[k, "false_positive_rate"] 
    y_1 <- roc01[k, "true_positive_rate"] 
    auc <- auc + y_1*(x_1-x_0)
}

auc





```{r}
 

pcm02 <- p0$my_principal_component_matrix$rotation

fn <- function(k) pcm02[1,] %*% pcm02[k,]

fn(2)

```

```{r}


im01 <- p0$my_incidence_matrix

```







